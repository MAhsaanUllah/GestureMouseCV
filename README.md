# üñ±Ô∏è Hand Gesture Virtual Mouse

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-4.6-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10-0097A7?style=for-the-badge&logo=google&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-22C55E?style=for-the-badge)
![Tests](https://img.shields.io/badge/Tests-Pytest-blue?style=for-the-badge&logo=pytest&logoColor=white)

**Control your computer with nothing but your hand ‚Äî no mouse required.**

*A real-time Computer Vision system that transforms webcam input into full mouse control using hand landmark detection and a trained gesture classifier.*

</div>

---

## üìñ Table of Contents

- [Demo](#-demo)
- [Key Features](#-key-features)
- [Technical Architecture](#-technical-architecture)
- [ML Model Details](#-ml-model-details)
- [Gesture Reference](#-gesture-reference)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Configuration](#-configuration)
- [Running Tests](#-running-tests)
- [Roadmap](#-roadmap)
- [Author](#-author)

---

## üé• Demo

> *Point your index finger at the camera and pinch to click.*

```
Webcam ‚Üí MediaPipe ‚Üí Landmark Extraction ‚Üí Gesture Classifier ‚Üí PyAutoGUI
   ‚Üë                                                                  ‚Üì
Frame Overlay ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Mouse / Keyboard Events ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

---

## ‚ú® Key Features

| Feature | Description |
|--------|-------------|
| **Real-Time Cursor Control** | Moves the OS mouse cursor via index-finger tracking at 30 fps |
| **Pinch-to-Click** | Thumb‚Äìindex pinch triggers left click; sustained pinch = drag |
| **Swipe Navigation** | Lateral hand swipe sends Alt+Right / Alt+Left (browser back/forward) |
| **Cursor Smoothing** | Exponential Moving Average (EMA) filter eliminates jitter |
| **On-Screen UI Panel** | Semi-transparent hover-aware buttons for quick folder/app access |
| **Configurable** | All thresholds, smoothing, and camera settings in one `config.py` |
| **CLI Interface** | `--device`, `--smoothing`, `--pinch-thresh`, `--debug` flags |
| **Structured Logging** | Console + rotating file log; debug mode for development |
| **Unit Tested** | Headless pytest suite covering gesture math & state machine |

---

## üèóÔ∏è Technical Architecture

```
hand_gesture_virtual_mouse/
‚îÇ
‚îú‚îÄ‚îÄ main.py                  ‚Üê Entry point (CLI, logging, error handling)
‚îú‚îÄ‚îÄ gesture_controller.py    ‚Üê Core engine (MediaPipe ‚Üí gesture ‚Üí OS events)
‚îú‚îÄ‚îÄ config.py                ‚Üê Centralised configuration (all magic numbers here)
‚îú‚îÄ‚îÄ train_model.py           ‚Üê ML training pipeline (standalone, re-runnable)
‚îÇ
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îî‚îÄ‚îÄ gesture_landmarks.csv   ‚Üê 42-feature landmark dataset (21 LM √ó x,y)
‚îÇ
‚îú‚îÄ‚îÄ model.h5                 ‚Üê Trained Keras model (gesture classifier)
‚îú‚îÄ‚îÄ scaler_mean.npy          ‚Üê StandardScaler mean (for inference normalisation)
‚îú‚îÄ‚îÄ scaler_scale.npy         ‚Üê StandardScaler scale
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_gesture_logic.py   ‚Üê Headless unit tests (no webcam required)
‚îÇ
‚îú‚îÄ‚îÄ logs/                    ‚Üê Auto-created; rotating log files
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Webcam      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  MediaPipe Hands ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 21 Landmarks    ‚îÇ
‚îÇ  (30 fps)    ‚îÇ    ‚îÇ  (real-time det) ‚îÇ    ‚îÇ (x, y, z each)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ  Gesture Interpreter  ‚îÇ
                                          ‚îÇ  ¬∑ pinch distance     ‚îÇ
                                          ‚îÇ  ¬∑ swipe dx ratio     ‚îÇ
                                          ‚îÇ  ¬∑ EMA smoothing      ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ    PyAutoGUI Events   ‚îÇ
                                          ‚îÇ  moveTo / click /     ‚îÇ
                                          ‚îÇ  mouseDown / hotkey   ‚îÇ
                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß† ML Model Details

| Property | Value |
|----------|-------|
| **Framework** | TensorFlow / Keras |
| **Input** | 42 features (21 hand landmarks √ó x, y coordinates) |
| **Architecture** | Dense(128, ReLU) ‚Üí Dropout(0.3) ‚Üí Dense(64, ReLU) ‚Üí Dropout(0.3) ‚Üí Softmax |
| **Normalisation** | StandardScaler (z-score) |
| **Optimiser** | Adam (lr=0.001) |
| **Callbacks** | EarlyStopping (patience=8) + ReduceLROnPlateau |
| **Train/Val Split** | 80 / 20 (stratified) |

> The classifier is trained offline via `train_model.py` and the saved `model.h5` is used by the real-time loop at inference time.

---

## ü§å Gesture Reference

| Gesture | Action |
|---------|--------|
| ‚òùÔ∏è **Index finger up** | Move cursor |
| ü§å **Pinch** (thumb + index close) | Left click |
| ‚úä **Hold pinch** | Click & drag |
| üëâ **Swipe right** | Alt + ‚Üí (forward) |
| üëà **Swipe left** | Alt + ‚Üê (back) |
| üëÜ **Cursor over panel button** + pinch | Open folder / Exit |

---

## üìÇ Project Structure

```
HandGesture_Virtual_Mouse/
‚îú‚îÄ‚îÄ main.py                 # Application entry point + CLI
‚îú‚îÄ‚îÄ gesture_controller.py   # GestureController class (core engine)
‚îú‚îÄ‚îÄ config.py               # All configuration in one place
‚îú‚îÄ‚îÄ train_model.py          # ML training pipeline
‚îú‚îÄ‚îÄ requirements.txt        # Pinned dependencies
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îî‚îÄ‚îÄ gesture_landmarks.csv
‚îú‚îÄ‚îÄ model.h5
‚îú‚îÄ‚îÄ scaler_mean.npy
‚îú‚îÄ‚îÄ scaler_scale.npy
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_gesture_logic.py
‚îî‚îÄ‚îÄ logs/                   # Auto-created at runtime
```

---

## üöÄ Quick Start

### Prerequisites

- **OS:** Windows 10 / 11
- **Python:** 3.9.x (64-bit) ‚Äî [Download](https://www.python.org/downloads/release/python-3910/)
- **Webcam:** Built-in or USB

> ‚ö†Ô∏è Python 3.10+ is **not** supported (TensorFlow + MediaPipe dependency conflict on that version).

### 1 ‚Äî Clone & Create Virtual Environment

```powershell
git clone https://github.com/MAhsaanUllah/HandGesture_Virtual_Mouse.git
cd HandGesture_Virtual_Mouse

py -3.9 -m venv cv_env
.\cv_env\Scripts\Activate.ps1
```

### 2 ‚Äî Install Dependencies

```powershell
# Install in the correct order to avoid conflicts
pip install numpy==1.23.5
pip install opencv-contrib-python==4.6.0.66
pip install pyautogui scikit-learn pandas tensorflow
pip install protobuf==3.20.3 absl-py flatbuffers attrs matplotlib
pip install mediapipe==0.10.5 --no-deps
pip install pytest  # for running tests
```

### 3 ‚Äî Run

```powershell
# Standard run
python main.py

# With options
python main.py --device 1 --smoothing 0.4 --debug

# Press Q to quit
```

---

## ‚öôÔ∏è Configuration

All tuneable parameters are in **`config.py`** ‚Äî no magic numbers anywhere else.

```python
# config.py (examples)

gesture.smoothing_alpha = 0.35    # cursor smoothness (0=max smooth, 1=raw)
gesture.pinch_threshold = 0.04    # pinch sensitivity (smaller = tighter pinch)
gesture.click_cooldown  = 0.60    # seconds between clicks
gesture.swipe_threshold = 0.40    # swipe sensitivity

hand.min_detection_confidence = 0.75
camera.device_index = 0           # webcam index
```

Or pass overrides directly via CLI:

```powershell
python main.py --smoothing 0.5 --pinch-thresh 0.03 --device 1 --debug
```

---

## üß™ Running Tests

The test suite runs **without a webcam or display** (ideal for CI pipelines):

```powershell
pytest tests/ -v
```

Example output:
```
tests/test_gesture_logic.py::TestEMA::test_alpha_zero_returns_prev   PASSED
tests/test_gesture_logic.py::TestEMA::test_alpha_one_returns_curr    PASSED
tests/test_gesture_logic.py::TestPinchDist::test_pythagorean         PASSED
tests/test_gesture_logic.py::TestGestureController::test_pinch_starts_drag  PASSED
...
```

---

## üó∫Ô∏è Roadmap

- [x] Real-time cursor control via index finger
- [x] Pinch-to-click and drag
- [x] Swipe navigation
- [x] Configurable thresholds
- [x] Cursor EMA smoothing
- [x] Unit test suite
- [ ] Right-click gesture (index + middle pinch)
- [ ] Scroll gesture (two-finger swipe)
- [ ] `collect_data.py` GUI for easy dataset collection
- [ ] Tkinter settings panel (adjust thresholds live)
- [ ] Package as standalone `.exe` with PyInstaller

---

## üõ†Ô∏è Tech Stack

- **[MediaPipe](https://google.github.io/mediapipe/)** ‚Äî Real-time hand landmark detection
- **[OpenCV](https://opencv.org/)** ‚Äî Frame capture, rendering, and UI overlay
- **[TensorFlow / Keras](https://www.tensorflow.org/)** ‚Äî Gesture classification model
- **[PyAutoGUI](https://pyautogui.readthedocs.io/)** ‚Äî Cross-platform OS mouse/keyboard control
- **[Scikit-learn](https://scikit-learn.org/)** ‚Äî StandardScaler + LabelEncoder (training)
- **[Pytest](https://pytest.org/)** ‚Äî Unit testing

---

## üë§ Author

**Muhammad Ahsaan Ullah**

- üíº LinkedIn: [linkedin.com/in/mahsaanullah](https://www.linkedin.com/in/mahsaanullah/)
- üêô GitHub: [@MAhsaanUllah](https://github.com/MAhsaanUllah)

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äî see the [LICENSE](LICENSE) file for details.

---

<div align="center">
‚≠ê If you found this useful, please star the repo!
</div>
